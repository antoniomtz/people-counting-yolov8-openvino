{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75257e56-c228-4d99-808f-2d6cf7576087",
   "metadata": {},
   "source": [
    "# People Counting System using YoloV8 and OpenVINOâ„¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e91cb-62ae-4049-b976-a89c7ebc0a59",
   "metadata": {},
   "source": [
    "In this project, I utlized YOLOV8 Object Counting class to develop a real-time people counting system using the YOLOv8 object detection model, optimized for Intel's OpenVINO toolkit to enhance inferencing speed. This system effectively monitors the number of individuals entering and exiting a room, leveraging the optimized YOLOv8 model for accurate people detection under varied conditions.\n",
    "\n",
    "By utilizing the OpenVINO runtime on Intel hardware, the system achieves significant improvements in processing speed, making it ideal for applications requiring real-time data, such as occupancy management and traffic flow control in public spaces and commercial settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86727720-ccae-471b-a8b3-b1b6e41f9e94",
   "metadata": {},
   "source": [
    "## Install pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee9d6e-9025-4179-8b0a-c8517168e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"openvino>=2024.0.0\" \"ultralytics==8.2.0\" \"torch>=2.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b9a300-05e9-4dfb-bfb1-ceba64ef5acc",
   "metadata": {},
   "source": [
    "## Download and convert yolov8 to OpenVINO Intermediate Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06461f51-72ab-401c-8c50-bb04c2f6ae88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "models_dir = Path(\"./models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "DET_MODEL_NAME = \"yolov8n\"\n",
    "\n",
    "det_model = YOLO(models_dir / f\"{DET_MODEL_NAME}.pt\")\n",
    "label_map = det_model.model.names\n",
    "\n",
    "res = det_model() # Need to make en empty call to initialize the model\n",
    "det_model_path = models_dir / f\"{DET_MODEL_NAME}_openvino_model/{DET_MODEL_NAME}.xml\"\n",
    "if not det_model_path.exists():\n",
    "    det_model.export(format=\"openvino\", dynamic=True, half=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c8848",
   "metadata": {},
   "source": [
    "## Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf6009-cd02-4fa3-9707-a883c5877dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import object_counter\n",
    "import cv2\n",
    "import time\n",
    "import collections\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import torch\n",
    "import openvino as ov\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def run_inference(source,device):\n",
    "    core = ov.Core()\n",
    "\n",
    "    det_ov_model = core.read_model(det_model_path)\n",
    "    ov_config = {} \n",
    "\n",
    "    if device.value != \"CPU\":\n",
    "        det_ov_model.reshape({0: [1, 3, 640, 640]})\n",
    "    if \"GPU\" in device.value or (\"AUTO\" in device.value and \"GPU\" in core.available_devices):\n",
    "        ov_config = {\"GPU_DISABLE_WINOGRAD_CONVOLUTION\": \"YES\"}\n",
    "    compiled_model = core.compile_model(det_ov_model, device.value, ov_config)\n",
    "    \n",
    "    def infer(*args):\n",
    "        result = compiled_model(args)\n",
    "        return torch.from_numpy(result[0])\n",
    "        \n",
    "    # Use openVINO as inference engine \n",
    "    det_model.predictor.inference = infer\n",
    "    det_model.predictor.model.pt = False\n",
    "    \n",
    "    try:\n",
    "        cap = cv2.VideoCapture(source)\n",
    "        assert cap.isOpened(), \"Error reading video file\"\n",
    "        \n",
    "        line_points = [(0, 300), (1080, 300)]  # line or region points\n",
    "        classes_to_count = [0]  # person is class 0 in the COCO dataset\n",
    "        \n",
    "        # Init Object Counter\n",
    "        counter = object_counter.ObjectCounter()\n",
    "        counter.set_args(view_img=False,                          \n",
    "                         reg_pts=line_points,\n",
    "                         classes_names=det_model.names,\n",
    "                         draw_tracks=True,\n",
    "                         line_thickness=2)\n",
    "        \n",
    "        # Processing time\n",
    "        processing_times = collections.deque(maxlen=200)  # Automatically discard oldest if over 200\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "                break\n",
    "        \n",
    "            start_time = time.time()\n",
    "            tracks = det_model.track(frame, persist=True, show=False,\n",
    "                                 classes=classes_to_count)\n",
    "            frame = counter.start_counting(frame, tracks)\n",
    "            stop_time = time.time()\n",
    "        \n",
    "            processing_times.append(stop_time - start_time)\n",
    "        \n",
    "            # Mean processing time [ms].\n",
    "            _, f_width = frame.shape[:2]\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 1000,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=2,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "    \n",
    "            # Get the counts.\n",
    "            counts = counter.out_counts\n",
    "            \n",
    "            # Define the text to display\n",
    "            text = f\"Total Count: {counts}\"\n",
    "            fontFace = cv2.FONT_HERSHEY_COMPLEX\n",
    "            fontScale = 0.75  # Adjust scale as needed\n",
    "            thickness = 2\n",
    "            \n",
    "            # Calculate the size of the text box\n",
    "            (text_width, text_height), _ = cv2.getTextSize(text, fontFace, fontScale, thickness)\n",
    "            \n",
    "            # Define the upper right corner for the text\n",
    "            top_right_corner = (frame.shape[1] - text_width - 40, 80)                     \n",
    "            # Draw the count of \"OUT\" on the frame\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=text,\n",
    "                org=(top_right_corner[0], top_right_corner[1]),\n",
    "                fontFace=fontFace,\n",
    "                fontScale=fontScale,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=thickness,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "        \n",
    "            # Show the frame\n",
    "            _, encoded_img = cv2.imencode(ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "            # Create an IPython image.\n",
    "            i = display.Image(data=encoded_img)\n",
    "            # Display the image in this notebook.\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(i)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542d587-7da3-4477-bfad-f9858fd752ae",
   "metadata": {},
   "source": [
    "## Run live demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bcc277-b1af-43d1-9683-a36166158bd4",
   "metadata": {},
   "source": [
    "Use webcam or video as inpute. Note: If you are running this notebook in the cloud, the webcam cannot be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c479d9-a401-43a4-af69-e4130513c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBCAM_INFERENCE = False\n",
    "\n",
    "if WEBCAM_INFERENCE:\n",
    "    VIDEO_SOURCE = 0  # Webcam\n",
    "else:\n",
    "    VIDEO_SOURCE = \"https://github.com/intel-iot-devkit/sample-videos/raw/master/people-detection.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f673ce4-a237-4ef5-adb8-f51c0fe5b3a9",
   "metadata": {},
   "source": [
    "## Select inference device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fe227-b67e-47f6-8742-5c5fe01edc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf1064-8ae5-4036-ba8d-7f330c164dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    source=VIDEO_SOURCE,    \n",
    "    device=device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
